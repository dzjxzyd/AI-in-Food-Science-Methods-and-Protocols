{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b716b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d3d03",
   "metadata": {},
   "source": [
    "### Predefined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a345ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for peptide sequences\n",
    "class PeptideDataset(Dataset):\n",
    "    def __init__(self, ds, tokenizer, seq_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ds (list): List of raw peptide sequences.\n",
    "            tokenizer: A tokenizer object that can map characters/tokens to IDs.\n",
    "            seq_len (int): Fixed sequence length for model input (after padding).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Define special tokens (convert them into tensor form)\n",
    "        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)  # Start of sequence\n",
    "        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)  # End of sequence\n",
    "        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)  # Padding\n",
    "        self.mask_token = torch.tensor([tokenizer.token_to_id(\"[MASK]\")], dtype=torch.int64) # Mask (for MLM)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of sequences in the dataset\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve one sequence, apply random masking, \n",
    "        then build encoder_input and label tensors with special tokens + padding.\n",
    "        \"\"\"\n",
    "        seq = self.ds[idx]  # Get raw sequence\n",
    "\n",
    "        # Apply masking strategy (15% replaced with [MASK]) \n",
    "        # Returns both masked sequence IDs and the original token IDs\n",
    "        masked_seq_ids, origi_seq_ids = random_mask(seq, self.tokenizer)\n",
    "\n",
    "        # Compute how many [PAD] tokens are needed after adding [SOS] and [EOS]\n",
    "        num_padding_tokens = self.seq_len - len(masked_seq_ids) - 2  \n",
    "        if num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sequence is too long for the specified seq_len\")\n",
    "\n",
    "        # Build encoder input: [SOS] + masked sequence + [EOS] + [PAD...]\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(masked_seq_ids, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Build label: [SOS] + original sequence + [EOS] + [PAD...]\n",
    "        # (the model should learn to predict original tokens from masked input)\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(origi_seq_ids, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Sanity check: ensure both sequences have fixed length = seq_len\n",
    "        assert encoder_input.size(0) == self.seq_len, \"encoder_input size mismatch\"\n",
    "        assert label.size(0) == self.seq_len, \"label size mismatch\"\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  \n",
    "            # Shape: (seq_len). Model input sequence with masked tokens and padding.\n",
    "\n",
    "            # \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  \n",
    "            \"encoder_mask\": (encoder_input == self.pad_token).bool(),  \n",
    "            # Shape: (1, 1, seq_len). Attention mask where 1 = valid token, 0 = pad.\n",
    "            # Two unsqueezes are added to match transformer attention dimensions.\n",
    "\n",
    "            \"label\": label,\n",
    "            # Shape: (seq_len). Original sequence (target labels for MLM).\n",
    "            \"seq\": seq\n",
    "        }\n",
    "# Helper function to apply random token masking (like BERT MLM)\n",
    "def random_mask(sentence, tokenizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentence (str): Raw sequence (string of characters).\n",
    "        tokenizer: Tokenizer to map characters to IDs.\n",
    "\n",
    "    Returns:\n",
    "        masked_seq_ids (list of int): Sequence IDs with 15% tokens replaced by [MASK].\n",
    "        origi_seq_ids (list of int): Original unmasked sequence IDs.\n",
    "    \"\"\"\n",
    "    origi_seq_ids = tokenizer.encode(sentence).ids  \n",
    "    masked_seq_ids = origi_seq_ids\n",
    "    # Sanity check: both masked and original sequences must have same length\n",
    "    assert len(masked_seq_ids) == len(origi_seq_ids), \\\n",
    "        \"Masked sequence length does not match original sequence length\"\n",
    "\n",
    "    return masked_seq_ids, origi_seq_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c14c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Fixed (non-learnable) sinusoidal positional encoding, as in\n",
    "    'Attention Is All You Need'. Precomputes a (max_len, d_model)\n",
    "    table and adds it to token embeddings at call time.\n",
    "\n",
    "    Args:\n",
    "        d_model: Embedding (model) dimension.\n",
    "        max_len: Maximum supported sequence length for precomputed table.\n",
    "        dropout: Dropout applied after adding positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Precompute sinusoidal table: shape (max_len, d_model)\n",
    "        # pe[pos, 2i]   = sin(pos / (10000^(2i/d_model)))\n",
    "        # pe[pos, 2i+1] = cos(pos / (10000^(2i/d_model)))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd  dimensions\n",
    "\n",
    "        # Add batch dim for broadcasting at runtime: (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register as a buffer so it moves with .to(device) but is not a parameter\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add positions to token embeddings.\n",
    "\n",
    "        Args:\n",
    "            x: Token embeddings, shape (B, S, D) = (batch, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B, S, D) with positions added, then dropout applied.\n",
    "        \"\"\"\n",
    "        # Slice the first S positions and add to x; no gradient through the table\n",
    "        x = x + self.pe[:, : x.size(1), :].requires_grad_(False)\n",
    "\n",
    "        # NOTE (optional): if you do NOT want to add positions to PAD rows,\n",
    "        # you can pass a (B, S) bool mask into forward and zero out pe on PAD:\n",
    "        # pe = self.pe[:, : x.size(1), :]\n",
    "        # pe = pe.masked_fill(key_padding_mask.unsqueeze(-1), 0.0)  # True=PAD\n",
    "        # x = x + pe\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "class PepBERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-style Transformer encoder for sequences (e.g., peptides).\n",
    "\n",
    "    Components:\n",
    "        - Token embedding with a defined padding_idx (PAD rows are zero and not updated).\n",
    "        - Fixed sinusoidal positional encoding (added to token embeddings).\n",
    "        - Stack of nn.TransformerEncoderLayer blocks (self-attention + FFN).\n",
    "        - Linear projection to vocabulary size (for MLM-style training).\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Size of the tokenizer vocabulary.\n",
    "        seq_len:    Max sequence length (drives positional table size).\n",
    "        pad_id:     Vocabulary ID used for PAD tokens.\n",
    "        d_model:    Embedding dimension.\n",
    "        n_heads:    Number of attention heads.\n",
    "        n_layers:   Number of encoder layers (depth).\n",
    "        d_ff:       Hidden size of the position-wise feed-forward (usually 4*d_model).\n",
    "        dropout:    Dropout probability used across the module.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        seq_len: int,\n",
    "        pad_id: int,\n",
    "        d_model: int = 160,\n",
    "        n_heads: int = 8,\n",
    "        n_layers: int = 6,\n",
    "        d_ff: int = 640,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embedding: PAD rows are always zero and are excluded from updates\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "\n",
    "        # Fixed positional encoding (sin/cos), shape added as (B, S, D)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, seq_len, dropout)\n",
    "\n",
    "        # Transformer encoder stack (each layer: MHA + FFN + residual + LayerNorm)\n",
    "        # batch_first=True => inputs are (B, S, D)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Final projection to logits over vocabulary (for masked language modeling)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_key_padding_mask: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            src: LongTensor token IDs, shape (B, S).\n",
    "            src_key_padding_mask: BoolTensor, shape (B, S),\n",
    "                                    True at PAD positions (to be ignored by attention).\n",
    "\n",
    "        Returns:\n",
    "            Logits over vocabulary: shape (B, S, vocab_size).\n",
    "        \"\"\"\n",
    "        # Embed tokens and scale by sqrt(d_model) (stabilizes training as in the paper)\n",
    "        x = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)  # (B, S, D)\n",
    "\n",
    "        # Add fixed sinusoidal positional encoding\n",
    "        x = self.pos_encoding(x)  # (B, S, D)\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # - nn.TransformerEncoder expects src_key_padding_mask with shape (B, S), True=PAD.\n",
    "        # - On Apple Silicon (MPS), passing this mask may trigger a nested-tensor path\n",
    "        #   not fully implemented. If you ever hit NotImplementedError on MPS, you can:\n",
    "        #   (A) Drop the mask on MPS:\n",
    "        #       if x.device.type == \"mps\": src_key_padding_mask = None\n",
    "        #   (B) Or pass a dummy attn_mask to disable that fast-path while keeping padding mask:\n",
    "        #       S = x.size(1)\n",
    "        #       dummy = torch.zeros((S, S), dtype=torch.bool, device=x.device)\n",
    "        #       x = self.encoder(x, mask=dummy, src_key_padding_mask=src_key_padding_mask)\n",
    "        #       return self.proj(x)\n",
    "\n",
    "        hidden_states = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # (B, S, D)\n",
    "\n",
    "        # Project hidden states to vocabulary logits (use CrossEntropyLoss with ignore_index=pad_id)\n",
    "        return hidden_states, self.proj(hidden_states)  # (B, S, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1d55a",
   "metadata": {},
   "source": [
    "### Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d17a2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PepBERT(\n",
       "  (embedding): Embedding(29, 160, padding_idx=1)\n",
       "  (pos_encoding): SinusoidalPositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=160, out_features=640, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=640, out_features=160, bias=True)\n",
       "        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj): Linear(in_features=160, out_features=29, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 1) Device selection =====\n",
    "# Automatically pick the best available device:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ===== 2) Load pretrained tokenizer & model (must match training setup) =====\n",
    "seq_len = 52  # maximum sequence length (with [SOS] and [EOS])\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "\n",
    "model = PepBERT(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    seq_len=seq_len,\n",
    "    pad_id=pad_id,\n",
    "    d_model=160,\n",
    "    n_heads=8,\n",
    "    n_layers=6,\n",
    "    d_ff=640,\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "# ===== 3) Load model checkpoint =====\n",
    "checkpoint = torch.load(\"tmodel_49.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()  # inference mode (disables dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac4067",
   "metadata": {},
   "source": [
    "### Define embedding function based on pretrianed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78770e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4) Helper: extract embeddings =====\n",
    "@torch.no_grad()\n",
    "def embed_sequences(dataloader, model, device):\n",
    "    \"\"\"\n",
    "    Extract one fixed-size embedding per sequence.\n",
    "\n",
    "    Steps:\n",
    "        - Forward pass through PepBERT → hidden states (B, S, D).\n",
    "        - Slice out the true amino acid tokens (excluding [SOS], [EOS], PAD).\n",
    "        - Mean-pool over token embeddings → single vector per sequence.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame of shape (N, d_model) with embeddings for all sequences.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for batch in tqdm(dataloader, desc=\"Embedding\"):\n",
    "        encoder_input = batch[\"encoder_input\"].to(device)    # (B, S)\n",
    "        encoder_mask  = batch[\"encoder_mask\"].to(device)     # (B, S), True=PAD\n",
    "        seq_list  = batch[\"seq\"]                         # list[str], raw sequences\n",
    "\n",
    "        # Forward pass. Model must return (hidden_states,logits).\n",
    "        hidden_states, _ = model(encoder_input, src_key_padding_mask=encoder_mask)  # (B, S, V)\n",
    "        # hidden: (B, S, d_model)\n",
    "    \n",
    "        # Compute mean embedding for each sequence in the batch\n",
    "        for i, raw_seq in enumerate(seq_list):\n",
    "            L = len(raw_seq)                   # true sequence length (without [SOS]/[EOS])\n",
    "            token_slice = hidden_states[i, 1:1+L, :]  # skip [SOS], take L tokens\n",
    "            pooled = token_slice.mean(dim=0).cpu().numpy()\n",
    "            rows.append(pooled)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53250964",
   "metadata": {},
   "source": [
    "### Embedding new dataset and model development for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9998d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|          | 0/4 [00:00<?, ?it/s]/hpc/group/youlab/zd75/envs/pubgo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Embedding: 100%|██████████| 4/4 [00:00<00:00, 15.21it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 112.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) Load datasets (Excel) =====\n",
    "train_df = pd.read_excel(\"QS_train.xlsx\", na_filter=False)\n",
    "test_df  = pd.read_excel(\"QS_test.xlsx\", na_filter=False)\n",
    "\n",
    "# Remove duplicates and sequences longer than 50 aa\n",
    "train_df = train_df.drop_duplicates(subset=[\"sequence\"])\n",
    "train_df = train_df[train_df[\"sequence\"].apply(len) <= 50].reset_index(drop=True)\n",
    "test_df  = test_df[test_df[\"sequence\"].apply(len) <= 50].reset_index(drop=True)\n",
    "\n",
    "# ===== 6) Build Dataset & DataLoader =====\n",
    "# Your PeptideDataset should return:\n",
    "#   - encoder_input: token IDs with [SOS]/[EOS]/PAD\n",
    "#   - encoder_mask : True at PAD positions\n",
    "#   - seq          : raw sequence (string)\n",
    "ds_train = PeptideDataset(train_df[\"sequence\"].tolist(), tokenizer, seq_len)\n",
    "ds_test  = PeptideDataset(test_df[\"sequence\"].tolist(),  tokenizer, seq_len)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=100, shuffle=False)\n",
    "dl_test  = DataLoader(ds_test,  batch_size=100, shuffle=False)\n",
    "\n",
    "# ===== 7) Extract embeddings for train and test sets =====\n",
    "emb_train = embed_sequences(dl_train, model, device)\n",
    "emb_test  = embed_sequences(dl_test,  model, device)\n",
    "emb_train.to_csv(\"peptide_bert_embed_train.csv\", index=False)\n",
    "emb_test.to_csv(\"peptide_bert_embed_test.csv\",  index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13c8934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': 0.9117647058823529, 'BACC': 0.9249999999999431, 'Sn': 0.9999999999999286, 'Sp': 0.8499999999999576, 'MCC': 0.8366600265340755, 'AUC': 0.9428571428571428}\n"
     ]
    }
   ],
   "source": [
    "# ===== 8) Train a logistic regression classifier =====\n",
    "y_train = train_df[\"label\"].to_numpy()\n",
    "y_test  = test_df[\"label\"].to_numpy()\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "clf.fit(emb_train, y_train)\n",
    "\n",
    "# ===== 9) Evaluate performance =====\n",
    "y_prob = clf.predict_proba(emb_test)[:, 1]      # predicted probability of class 1\n",
    "y_pred = (y_prob >= 0.5).astype(int)            # binary prediction\n",
    "\n",
    "# Confusion matrix elements\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Compute metrics\n",
    "ACC  = (TP+TN) / (TP+TN+FP+FN)\n",
    "Sn   = TP / (TP+FN+1e-12)        # sensitivity (recall)\n",
    "Sp   = TN / (TN+FP+1e-12)        # specificity\n",
    "BACC = 0.5 * (Sn + Sp)           # balanced accuracy\n",
    "MCC  = (TP*TN - FP*FN) / math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN) + 1e-12)\n",
    "AUC  = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "metrics = dict(ACC=ACC, BACC=BACC, Sn=Sn, Sp=Sp, MCC=MCC, AUC=AUC)\n",
    "print(metrics)\n",
    "\n",
    "# Save metrics\n",
    "pd.DataFrame([metrics]).to_csv(\"performance_report.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58e85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
